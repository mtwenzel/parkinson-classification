{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of PPMI-InceptionV3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtwenzel/parkinson-classification/blob/master/PPMI_InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF59emVMi9yl",
        "colab_type": "text"
      },
      "source": [
        "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/mtwenzel/parkinson-classification/blob/master/PPMI-InceptionV3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ-adKBAi9yo",
        "colab_type": "text"
      },
      "source": [
        "# Classifying PPMI DAT scans into Parkinson's Disease and Healthy Controls\n",
        "\n",
        "Licensed under [this](LICENSE) license.\n",
        "\n",
        "This notebook shows how we performed the experiment to fine-tune the Inception V3 classifier to distinguish patients with and without signs of Parkinson's disease.\n",
        "\n",
        "The notebook is optimized to work with Google Colab. It hides all code by default so that you can run it sequentially from top to bottom. If you want to see the implementation, just double-click into any cell, and it will open in a side-by-side view.\n",
        "\n",
        "This code is part of the publication \n",
        "> Publication reference and [link](dummy-link-not-working-yet) to be inserted after publishing.\n",
        "\n",
        "The data are a derivative of the DAT scans available from the [PPMI repository](https://www.ppmi-info.org/access-data-specimens/download-data/). They were processed to represent the central 5 slices of the putamen in one slice by averaging them. For details, please refer to the paper.\n",
        "\n",
        "The data was then split randomly into a training and a validation set. As we tested the performance on an independant test set drawn from clinical routine which cannot be published, this notebook does not contain testing of the trained classifier.\n",
        "\n",
        "The data as used in the publication can be downloaded here:\n",
        "\n",
        "If you want to run the notebook from Google Colab, put the data into your Google Drive, and adapt the path below in the respective cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK46L_mVmW_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Download and unzip the data. {display-mode:'form'}\n",
        "#@markdown The data resides in the GitHub repository. For Hosted Runtime users, it is temporarily downloaded to the runtime's location.\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "zipurl = 'https://github.com/mtwenzel/parkinson-classification/raw/master/data/PPMI-classification.zip'\n",
        "zipresp = urlopen(zipurl)\n",
        "tempzip = open(\"PPMI-classification.zip\", \"wb\")\n",
        "tempzip.write(zipresp.read())\n",
        "tempzip.close()\n",
        "print(\"download complete, extracting...\")\n",
        "\n",
        "zf = ZipFile(\"PPMI-classification.zip\")\n",
        "zf.extractall(path = 'data/')\n",
        "zf.close()\n",
        "print(\"... done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAdkJ6Xhi9yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Check GPU availability {display-mode: 'form'}\n",
        "#@markdown If you don't see a GPU in Colab, activate it by selecting *Runtime -> Change Runtime Type* in the menu.\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print('')\n",
        "print('Detailed information:')\n",
        "print('---------------------')\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w4QcbOqi9yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports. {display-mode:'form'}\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,Dense,GlobalAveragePooling2D,Flatten,concatenate,BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import InceptionV3,DenseNet121\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Visualize the Train/Val loss\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlJrs0Mti9yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Set the data generators. {display-mode:'form', run: \"auto\"}\n",
        "#@markdown Data augmentation choices. Cell runs automatically if anything is changed.\n",
        "shear_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "zoom_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "width_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "height_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:5}\n",
        "horizontal_flip = True #@param {type:\"boolean\"}\n",
        "vertical_flip = False #@param {type:\"boolean\"}\n",
        "#@markdown Data source (No need to change if the download succeeded.)\n",
        "data_directory = '/content/data/PPMI-classification/' #@param ['z:/Data/Parkinson_DATScans UKE/full_ppmi_data/png/', '/content/drive/My Drive/MEVIS/Data/PPMI-classification/'] {allow-input: true}\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "    shear_range=shear_range,\n",
        "    zoom_range=zoom_range,\n",
        "    width_shift_range=width_shift_range,\n",
        "    height_shift_range=height_shift_range,\n",
        "    rotation_range=rotation_range,\n",
        "    horizontal_flip=horizontal_flip,\n",
        "    vertical_flip=vertical_flip) \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(os.path.join(data_directory, 'all_2d_train'), # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(109,91),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=64,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "\n",
        "# Data Generator for validation without data augmentation!\n",
        "val_datagen   = ImageDataGenerator(rescale=1./255) \n",
        "val_generator = val_datagen.flow_from_directory(os.path.join(data_directory, 'all_2d_val'), # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(109,91),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=64,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9UeQBnwi9y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Set up the pretrained model, and add dense layers. {display-mode:'form', run: \"auto\"}\n",
        "#@markdown Set up the trainable dense layers. Further options include BatchNorm (provides regularization), DropOut (also, for normalization, but should not be used together with BatchNorm), and GlobalAveragePooling as an alternative to simple flattening. Did not work well in our experiments.  Cell runs automatically if anything is changed.\n",
        "first_dense_layer_neurons  = 1024 #@param {type:\"integer\"}\n",
        "second_dense_layer_neurons = 256 #@param {type:\"integer\"}\n",
        "use_global_average_pooling = False #@param {type:\"boolean\"}\n",
        "use_batch_norm             = True #@param {type:\"boolean\"}\n",
        "use_drop_out               = False  #@param {type:\"boolean\"}\n",
        "pretrained_model           = 'Inception V3' #@param [\"Inception V3\", \"DenseNet 121\"]\n",
        "optimizer                  = 'adam' #@param ['adam', 'adagrad', 'adadelta', 'sgd'] {allow-input: true}\n",
        "\n",
        "if pretrained_model == 'Inception V3':\n",
        "    base_model=InceptionV3(weights='imagenet',include_top=False, input_shape=(109,91,3)) \n",
        "else:\n",
        "    base_model=DenseNet121(weights='imagenet',include_top=False, input_shape=(109,91,3)) \n",
        "\n",
        "x=base_model.output\n",
        "\n",
        "if use_global_average_pooling == True:\n",
        "    x=GlobalAveragePooling2D()(x)\n",
        "else:\n",
        "    x=Flatten()(x)\n",
        "\n",
        "if use_batch_norm:\n",
        "    x = BatchNormalization()(x)\n",
        "if use_drop_out:\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "x = Dense(first_dense_layer_neurons,activation='relu')(x)\n",
        "\n",
        "if use_batch_norm:\n",
        "    x = BatchNormalization()(x)\n",
        "if use_drop_out:\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "x = Dense(second_dense_layer_neurons,activation='relu')(x)\n",
        "\n",
        "if use_batch_norm:\n",
        "    x = BatchNormalization()(x)\n",
        "if use_drop_out:\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "preds = Dense(2,activation='softmax')(x) # final layer with softmax activation\n",
        "\n",
        "model = Model(inputs=base_model.input,outputs=preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZbTepTui9y4",
        "colab_type": "text"
      },
      "source": [
        "## First pass: train added dense layers\n",
        "Set up the trainable parameters. \n",
        "\n",
        "First train only the top layers (which were randomly initialized), i.e. freeze all convolutional InceptionV3 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI9967K8i9y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Set up trainable parameters {display-mode:'form'}\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "if optimizer in ['adam', 'adagrad', 'adadelta', 'sgd']: # standard settings\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['accuracy']) # categorical crossentropy would also do...\n",
        "else:\n",
        "    from tensorflow.keras.optimizers import SGD\n",
        "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "trainable_count = int(\n",
        "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
        "non_trainable_count = int(\n",
        "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
        "\n",
        "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
        "print('Trainable params: {:,}'.format(trainable_count))\n",
        "print('Non-trainable params: {:,}'.format(non_trainable_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F08l0mwmi9y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run training {display-mode:'form'}\n",
        "\n",
        "history = model.fit_generator(generator=train_generator,\n",
        "                              steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
        "                              epochs=100, # Originally, 500 epochs!\n",
        "                             validation_data=val_generator,\n",
        "                             validation_steps=val_generator.n//val_generator.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPuHoj1_i9y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot train and validation loss/accuracy {display-mode:'form'}\n",
        "\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Depending on the version of TF/Keras, the metric is either stored as 'acc' or 'accuracy'. This is not checked here.\n",
        "plt.plot(history.history['acc'], label='train acc')\n",
        "plt.plot(history.history['val_acc'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcWI3vZi9zB",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tune last convolutional layers\n",
        "\n",
        "The top layers should have converged, and it would be good practice to fine-tune the top convolutional layers. \n",
        "\n",
        "Print the layers with indices. \n",
        "\n",
        "We chose to fine-tune the top 2 inception blocks, i.e. we will freeze the first 249 layers and unfreeze the rest. Afterwards, the model needs to be recompiled. \n",
        "\n",
        "Note that this will not change the trained parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDqm1EFmi9zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Set up trainable parameters {display-mode:'form'}\n",
        "\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)\n",
        "\n",
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "trainable_count = int(\n",
        "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
        "non_trainable_count = int(\n",
        "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
        "\n",
        "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
        "print('Trainable params: {:,}'.format(trainable_count))\n",
        "print('Non-trainable params: {:,}'.format(non_trainable_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlQ9Mjigi9zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run training {display-mode:'form'}\n",
        "\n",
        "history_finetune = model.fit_generator(generator=train_generator,\n",
        "                              steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
        "                              epochs=100,\n",
        "                             validation_data=val_generator,\n",
        "                             validation_steps=val_generator.n//val_generator.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQBk1R-9i9zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot train and validation loss/accuracy {display-mode:'form'}\n",
        "plt.plot(history_finetune.history['loss'], label='train loss')\n",
        "plt.plot(history_finetune.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history_finetune.history['acc'], label='train acc')\n",
        "plt.plot(history_finetune.history['val_acc'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}