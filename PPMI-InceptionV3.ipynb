{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/mtwenzel/parkinson-classification/blob/master/PPMI-InceptionV3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying PPMI DAT scans into Parkinson's Disease and Healthy Controls\n",
    "\n",
    "Licensed under [this](LICENSE) license.\n",
    "\n",
    "This notebook shows how we performed the experiment to fine-tune the Inception V3 classifier to distinguish patients with and without signs of Parkinson's disease.\n",
    "\n",
    "The notebook is optimized to work with Google Colab.\n",
    "\n",
    "It is part of the publication \n",
    "> Publication reference and [link](Link) to be inserted after publishing.\n",
    "\n",
    "The data are a derivative of the DAT scans available from the [PPMI repository](https://www.ppmi-info.org/access-data-specimens/download-data/). Roughly, they were processed to represent the central 5 slices of the putamen in one slice by averaging them. The data was then split randomly into a training and a validation set. As we tested the performance on an independant test set drawn from clinical routine which cannot be published, this notebook does not contain testing of the trained classifier.\n",
    "\n",
    "The data as used in the publication can be downloaded here:\n",
    "\n",
    "If you want to run the notebook from Google Colab, put the data into your Google Drive, and adapt the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Connect Google Drive if running in Colab. {display-mode:'form'}\n",
    "#@markdown You will be asked to navigate to a Google site to log in and allow access to your drive.\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU availability {display-mode: 'form'}\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print('')\n",
    "print('Detailed information:')\n",
    "print('---------------------')\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports. {display-mode:'form'}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Dense,GlobalAveragePooling2D,Flatten,concatenate,BatchNormalization, Dropout\n",
    "from tensorflow.keras.applications import InceptionV3,DenseNet121\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Visualize the Train/Val loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set the data generators. {display-mode:'form', run: \"auto\"}\n",
    "#@markdown Data augmentation choices.\n",
    "shear_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "zoom_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "width_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "height_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:5}\n",
    "horizontal_flip = True #@param {type:\"boolean\"}\n",
    "vertical_flip = False #@param {type:\"boolean\"}\n",
    "#@markdown Data source (find your Google Drive path on the left in Colab!)\n",
    "data_directory = 'z:/Data/Parkinson_DATScans UKE/full_ppmi_data/png/' #@param ['z:/Data/Parkinson_DATScans UKE/full_ppmi_data/png/', '/content/drive/My Drive/MEVIS/Data/PPMI-classification/'] {allow-input: true}\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    rotation_range=rotation_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip=vertical_flip) \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(os.path.join(data_directory, 'all_2d_train'), # this is where you specify the path to the main data folder\n",
    "                                                 target_size=(109,91),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=64,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "# Data Generator for validation without data augmentation!\n",
    "val_datagen   = ImageDataGenerator(rescale=1./255) \n",
    "val_generator = val_datagen.flow_from_directory(os.path.join(data_directory, 'all_2d_val'), # this is where you specify the path to the main data folder\n",
    "                                                 target_size=(109,91),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=64,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up the pretrained model, and add dense layers. {display-mode:'form', run: \"auto\"}\n",
    "#@markdown Set up the trainable dense layers. Further options include BatchNorm (provides regularization), DropOut (also, for normalization, but should not be used together with BatchNorm), and GlobalAveragePooling as an alternative to simple flattening. Did not work well in our experiments.\n",
    "first_dense_layer_neurons  = 1024 #@param {type:\"integer\"}\n",
    "second_dense_layer_neurons = 256 #@param {type:\"integer\"}\n",
    "use_global_average_pooling = False #@param {type:\"boolean\"}\n",
    "use_batch_norm             = True #@param {type:\"boolean\"}\n",
    "use_drop_out               = False  #@param {type:\"boolean\"}\n",
    "pretrained_model           = 'Inception V3' #@param [\"Inception V3\", \"DenseNet 121\"]\n",
    "optimizer                  = 'adam' #@param ['adam', 'adagrad', 'adadelta', 'sgd'] {allow-input: true}\n",
    "\n",
    "if pretrained_model == 'Inception V3':\n",
    "    base_model=InceptionV3(weights='imagenet',include_top=False, input_shape=(109,91,3)) \n",
    "else:\n",
    "    base_model=DenseNet121(weights='imagenet',include_top=False, input_shape=(109,91,3)) \n",
    "\n",
    "x=base_model.output\n",
    "\n",
    "if use_global_average_pooling == True:\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "else:\n",
    "    x=Flatten()(x)\n",
    "\n",
    "if use_batch_norm:\n",
    "    x = BatchNormalization()(x)\n",
    "if use_drop_out:\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "x = Dense(first_dense_layer_neurons,activation='relu')(x)\n",
    "\n",
    "if use_batch_norm:\n",
    "    x = BatchNormalization()(x)\n",
    "if use_drop_out:\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "x = Dense(second_dense_layer_neurons,activation='relu')(x)\n",
    "\n",
    "if use_batch_norm:\n",
    "    x = BatchNormalization()(x)\n",
    "if use_drop_out:\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "preds = Dense(2,activation='softmax')(x) # final layer with softmax activation\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title First pass: train added dense layers {display-mode:'form'}\n",
    "#@markdown Set up the trainable parameters\n",
    "#@narkdown First train only the top layers (which were randomly initialized), i.e. freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "if optimizer in ['adam', 'adagrad', 'adadelta', 'sgd']: # standard settings\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['accuracy']) # categorical crossentropy would also do...\n",
    "else:\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "non_trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                              epochs=100, # Originally, 500 epochs!\n",
    "                             validation_data=val_generator,\n",
    "                             validation_steps=val_generator.n//val_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train and validation loss/accuracy {display-mode:'form'}\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Depending on the version of TF/Keras, the metric is either stored as 'acc' or 'accuracy'. This is not checked here.\n",
    "plt.plot(history.history['acc'], label='train acc')\n",
    "plt.plot(history.history['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Fine-tune last convolutional layers {display-mode:'form'}\n",
    "#@markdown The top layers should have converged, and it would be good practice to fine-tune the top convolutional layers. Print the layers with indices. We chose to fine-tune the top 2 inception blocks, i.e. we will freeze the first 249 layers and unfreeze the rest. Afterwards, the model needs to be recompiled. Note that this will not change the trained parameters.\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "non_trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "history_finetune = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                              epochs=100,\n",
    "                             validation_data=val_generator,\n",
    "                             validation_steps=val_generator.n//val_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train and validation loss/accuracy {display-mode:'form'}\n",
    "plt.plot(history_finetune.history['loss'], label='train loss')\n",
    "plt.plot(history_finetune.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_finetune.history['acc'], label='train acc')\n",
    "plt.plot(history_finetune.history['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
